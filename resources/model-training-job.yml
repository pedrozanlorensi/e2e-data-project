resources:
  jobs:
    model_training_job:
      name: "Iris Model Training job - ${var.environment}"
      description: "Job to train iris classification model"
      
      # Email notifications on failure
      email_notifications:
        on_failure:
          - "your.name@address.com"
        no_alert_for_skipped_runs: false
      
      # Job parameters
      parameters:
        - name: "catalog_name"
          default: "${var.catalog_name}"
      
      # If you want to define a shared job cluster for data preprocessing,
      # uncomment the following section - this is an example that works for Azure Databricks
      # job_clusters:
      #   - job_cluster_key: "training_cluster"
      #     new_cluster:
      #       autoscale:
      #         min_workers: 1
      #         max_workers: 4
      #       spark_version: "15.4.x-scala2.12"
      #       node_type_id: "Standard_D4s_v3"
      #       custom_tags:
      #         Environment: "${var.environment}"
      #         Project: "e2e-data-project"
      #         Purpose: "model-training"

      # Job tasks
      tasks:
        - task_key: "train_model"
          description: "Train iris classification model"
          
          # Reference the shared job cluster if you want to use it
          # job_cluster_key: "training_cluster"
          
          # Notebook task
          notebook_task:
            notebook_path: "../Notebooks/2_ModelTrainingAndDeployment/model-training.ipynb"
            base_parameters:
              catalog_name: "{{job.parameters.catalog_name}}"
      
      # Timeout and retry settings
      timeout_seconds: 3600
      max_concurrent_runs: 1
      
      # Tags for organization
      tags:
        Environment: "${var.environment}"
        Project: "e2e-data-project" 